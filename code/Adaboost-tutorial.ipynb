{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95cc08d6",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e4fd126",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-cb273a8676ea>:15: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from numpy.core.umath_tests import inner1d\n",
    "from copy import deepcopy\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import classification_report\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        \n",
    "df_train = pd.read_csv('C:/Users/sq/Project/IOT/IOT-Code/iot_device_train.csv')\n",
    "df_test = pd.read_csv('C:/Users/sq/Project/IOT/IOT-Code/iot_device_test.csv')\n",
    "\n",
    "df = pd.concat([df_train,df_test],ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff8e31f",
   "metadata": {},
   "source": [
    "## Delete unique data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3036ca4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove = []\n",
    "for col in df.columns:\n",
    "    if len(df[col].unique()) == 1:\n",
    "        to_remove.append(col)\n",
    "df = df.drop(to_remove, axis=1)\n",
    "\n",
    "X = df.drop('device_category', axis=1)\n",
    "y = df['device_category']\n",
    "#y=pd.get_dummies(y)#Convert all categories to numeric types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "343ad7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodeLabel(data):\n",
    "    listLable=[]\n",
    "    #这里我的标签的名字全都打成了lable，我知道标签的英文是label，如果大家实在看不惯想改过来的话记得前面加载的csv文件的开头的lable也改成label\n",
    "    for lable in data:\n",
    "        listLable.append(lable)\n",
    "    #到这里都是把lable整合到一起，下面是规格化处理\n",
    "    le = LabelEncoder()\n",
    "    resultLable=le.fit_transform(listLable)\n",
    "    return resultLable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ac5ad93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 4, 4, ..., 8, 8, 8], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=encodeLabel(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522fb5fb",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9630167",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "#normalization\n",
    "input_features = preprocessing.StandardScaler().fit_transform(X)\n",
    "#split\n",
    "X_train, X_test, y_train, y_test = train_test_split(input_features,y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1255b4d0",
   "metadata": {},
   "source": [
    "## Sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac8f6870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.82      0.75        57\n",
      "           1       0.79      0.94      0.86        48\n",
      "           2       0.00      0.00      0.00        48\n",
      "           3       0.50      0.93      0.65        67\n",
      "           4       1.00      0.14      0.24        58\n",
      "           5       0.20      0.45      0.28        64\n",
      "           6       0.00      0.00      0.00        57\n",
      "           7       0.59      1.00      0.74        67\n",
      "           8       0.86      0.71      0.78        69\n",
      "           9       0.00      0.00      0.00        35\n",
      "\n",
      "    accuracy                           0.54       570\n",
      "   macro avg       0.46      0.50      0.43       570\n",
      "weighted avg       0.49      0.54      0.46       570\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sq\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\sq\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\sq\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "model_ada =  AdaBoostClassifier()\n",
    "model_ada.fit(X_train, y_train)\n",
    "fit_pred = model_ada.predict(X_test)\n",
    " \n",
    "print(classification_report(y_test, fit_pred))\n",
    "#report = classification_report(y_test, fit_pred,output_dict=True)\n",
    "#report= pd.DataFrame(report).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf54c65",
   "metadata": {},
   "source": [
    "Tree structure as a base classifier does not work well for integration here, therefore, we try to replace the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e63ea45",
   "metadata": {},
   "source": [
    "## Train a Adaboost classifier from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd5b93d",
   "metadata": {},
   "source": [
    "### 1.Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0944637d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaClassifier(object):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        if kwargs and args:\n",
    "            raise ValueError(\n",
    "                '''AdaBoostClassifier can only be called with the following keyword arguments：\n",
    "                base_estimator,n_estimators,learning_rate,algorithm,random_state\n",
    "                ''')\n",
    "        allowed_keys = ['base_estimator', 'n_estimators', 'learning_rate', 'algorithm', 'random_state']\n",
    "        keywords_used = kwargs.keys()\n",
    "        for keyword in keywords_used:\n",
    "            if keyword not in allowed_keys:\n",
    "                raise ValueError(keyword + \":  Wrong keyword used --- check spelling\")\n",
    "\n",
    "        n_estimators = 50\n",
    "        learning_rate = 1\n",
    "        algorithm = 'SAMME.R'\n",
    "        random_state = None\n",
    "\n",
    "        if kwargs and not args:\n",
    "            if 'base_estimator' in kwargs:\n",
    "                base_estimator = kwargs.pop('base_estimator')\n",
    "            else:\n",
    "                raise ValueError('''base_estimator can not be None''')\n",
    "            if 'n_estimators' in kwargs: n_estimators = kwargs.pop('n_estimators')\n",
    "            if 'learning_rate' in kwargs: learning_rate = kwargs.pop('learning_rate')\n",
    "            if 'algorithm' in kwargs: algorithm = kwargs.pop('algorithm')\n",
    "            if 'random_state' in kwargs: random_state = kwargs.pop('random_state')\n",
    "\n",
    "        self.base_estimator_ = base_estimator\n",
    "        # self.base_estimator_ = SVC(probability=True)\n",
    "        self.n_estimators_ = n_estimators\n",
    "        self.learning_rate_ = learning_rate\n",
    "        self.algorithm_ = algorithm\n",
    "        self.random_state_ = random_state\n",
    "        self.estimators_ = list()\n",
    "        #self.estimators_ =base_estimator\n",
    "\n",
    "        self.estimator_weights_ = np.zeros(self.n_estimators_)\n",
    "        self.estimator_errors_ = np.ones(self.n_estimators_)\n",
    "\n",
    "\n",
    "    def _samme_proba(self, estimator, n_classes, X):#model,1\n",
    "        \"\"\"Calculate algorithm 4, step 2, equation c) of Zhu et al [1].\n",
    "\n",
    "        References\n",
    "        ----------\n",
    "        .. [1] J. Zhu, H. Zou, S. Rosset, T. Hastie, \"Multi-class AdaBoost\", 2009.\n",
    "\n",
    "        \"\"\"\n",
    "        proba = estimator.predict_proba(X)#return probability\n",
    "        #print('\\n', n_classes)\n",
    "        # Displace zero probabilities so the log is defined.\n",
    "        # Also fix negative elements which may occur with\n",
    "        # negative sample weights.\n",
    "        proba[proba < np.finfo(proba.dtype).eps] = np.finfo(proba.dtype).eps\n",
    "        log_proba = np.log(proba)\n",
    "\n",
    "        return (n_classes - 1) * (log_proba - (1. / n_classes)\n",
    "                                  * log_proba.sum(axis=1)[:, np.newaxis])\n",
    "\n",
    "\n",
    "    def fit(self, X, y):#train\n",
    "        self.n_samples = X.shape[0]\n",
    "        #self.n_samples = X_train.shape[0]\n",
    "        # There is hidden trouble for classes, here the classes will be sorted.\n",
    "        self.classes_ = np.array(sorted(list(set(y))))\n",
    "        \n",
    "        self.n_classes_ = len(self.classes_)#The number of categories\n",
    "        for iboost in range(self.n_estimators_):#num of base estimators\n",
    "            if iboost == 0:\n",
    "                sample_weight = np.ones(self.n_samples) / self.n_samples # Weight Initialization：1/N\n",
    "\n",
    "            sample_weight, estimator_weight, estimator_error = self.boost(X,y, sample_weight)#\n",
    "\n",
    "            # early stop\n",
    "            if estimator_error == None:\n",
    "                break\n",
    "\n",
    "            # append error and weight\n",
    "            self.estimator_errors_[iboost] = estimator_error\n",
    "            self.estimator_weights_[iboost] = estimator_weight\n",
    "\n",
    "            if estimator_error <= 0:\n",
    "                break\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "    def boost(self, X, y, sample_weight):\n",
    "        if self.algorithm_ == 'SAMME':\n",
    "            return self.discrete_boost(X, y, sample_weight)\n",
    "        elif self.algorithm_ == 'SAMME.R':\n",
    "            return self.real_boost(X, y, sample_weight)\n",
    "\n",
    "    def real_boost(self, X, y, sample_weight):\n",
    "        estimator = deepcopy(self.base_estimator_)#Deep copy produces a new object estimator store base classifier\n",
    "        if self.random_state_:\n",
    "            estimator.set_params(random_state=1)\n",
    "\n",
    "        estimator.fit(X, y, sample_weight=sample_weight)#train\n",
    "\n",
    "        y_pred = estimator.predict(X)#\n",
    "        incorrect = y_pred != y\n",
    "        estimator_error = np.dot(incorrect, sample_weight) / np.sum(sample_weight, axis=0)\n",
    "\n",
    "        # if worse than random guess, stop boosting\n",
    "        if estimator_error >= 1.0 - 1 / self.n_classes_:#1- 1/10\n",
    "            return None, None, None\n",
    "\n",
    "        y_predict_proba = estimator.predict_proba(X)\n",
    "        # repalce zero\n",
    "        y_predict_proba[y_predict_proba < np.finfo(y_predict_proba.dtype).eps] = np.finfo(y_predict_proba.dtype).eps\n",
    "\n",
    "        y_codes = np.array([-1. / (self.n_classes_ - 1), 1.])\n",
    "        y_coding = y_codes.take(self.classes_ == y[:, np.newaxis])\n",
    "\n",
    "        # for sample weight update\n",
    "        intermediate_variable = (-1. * self.learning_rate_ * (((self.n_classes_ - 1) / self.n_classes_) *\n",
    "                                                              inner1d(y_coding, np.log(\n",
    "                                                                  y_predict_proba))))  #dot iterate for each row\n",
    "\n",
    "        # update sample weight\n",
    "        sample_weight *= np.exp(intermediate_variable)\n",
    "\n",
    "        sample_weight_sum = np.sum(sample_weight, axis=0)\n",
    "        if sample_weight_sum <= 0:\n",
    "            return None, None, None\n",
    "\n",
    "        # normalize sample weight\n",
    "        sample_weight /= sample_weight_sum\n",
    "\n",
    "        # append the estimator\n",
    "        self.estimators_.append(estimator)\n",
    "\n",
    "        return sample_weight, 1, estimator_error\n",
    "\n",
    "\n",
    "    def discrete_boost(self, X, y, sample_weight):\n",
    "        estimator = deepcopy(self.base_estimator_)\n",
    "        if self.random_state_:\n",
    "            estimator.set_params(random_state=1)\n",
    "\n",
    "        estimator.fit(X, y, sample_weight=sample_weight)\n",
    "\n",
    "        y_pred = estimator.predict(X)\n",
    "        incorrect = y_pred != y\n",
    "        estimator_error = np.dot(incorrect, sample_weight) / np.sum(sample_weight, axis=0)\n",
    "\n",
    "        # if worse than random guess, stop boosting\n",
    "        if estimator_error >= 1 - 1 / self.n_classes_:\n",
    "            return None, None, None\n",
    "\n",
    "        # update estimator_weight\n",
    "        estimator_weight = self.learning_rate_ * np.log((1 - estimator_error) / estimator_error) + np.log(\n",
    "            self.n_classes_ - 1)\n",
    "\n",
    "        if estimator_weight <= 0:\n",
    "            return None, None, None\n",
    "\n",
    "        # update sample weight\n",
    "        sample_weight *= np.exp(estimator_weight * incorrect)\n",
    "\n",
    "        sample_weight_sum = np.sum(sample_weight, axis=0)\n",
    "        if sample_weight_sum <= 0:\n",
    "            return None, None, None\n",
    "\n",
    "        # normalize sample weight\n",
    "        sample_weight /= sample_weight_sum\n",
    "\n",
    "        # append the estimator\n",
    "        self.estimators_.append(estimator)\n",
    "\n",
    "        return sample_weight, estimator_weight, estimator_error\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_classes = self.n_classes_\n",
    "        classes = self.classes_[:, np.newaxis]\n",
    "        pred = None\n",
    "\n",
    "        if self.algorithm_ == 'SAMME.R':\n",
    "            # The weights are all 1. for SAMME.R\n",
    "            pred = sum(self._samme_proba(estimator, X) for estimator in self.estimators_)\n",
    "        else:  # self.algorithm == \"SAMME\"\n",
    "            pred = sum((estimator.predict(X) == classes).T * w\n",
    "                       for estimator, w in zip(self.estimators_,\n",
    "                                               self.estimator_weights_))\n",
    "\n",
    "        pred /= self.estimator_weights_.sum()\n",
    "        if n_classes == 2:\n",
    "            pred[:, 0] *= -1\n",
    "            pred = pred.sum(axis=1)\n",
    "            return self.classes_.take(pred > 0, axis=0)\n",
    "\n",
    "        return self.classes_.take(np.argmax(pred, axis=1), axis=0)\n",
    "\n",
    "\n",
    "    def predict_proba(self,X):\n",
    "        n_classes = self.n_classes_\n",
    "        if self.algorithm_ == 'SAMME.R':\n",
    "            # The weights are all 1. for SAMME.R\n",
    "            proba = sum(self._samme_proba(estimator, self.n_classes_, X)\n",
    "                        for estimator in self.estimators_)\n",
    "        else:  # self.algorithm == \"SAMME\"\n",
    "            proba = sum(estimator.predict_proba(X) * w\n",
    "                        for estimator, w in zip(self.estimators_,\n",
    "                                                self.estimator_weights_))\n",
    "\n",
    "        proba /= self.estimator_weights_.sum()\n",
    "        proba = np.exp((1. / (n_classes - 1)) * proba)\n",
    "        normalizer = proba.sum(axis=1)[:, np.newaxis]\n",
    "        normalizer[normalizer == 0.0] = 1.0\n",
    "        proba /= normalizer#归一化\n",
    "        \n",
    "        #pre_label=predict_label(proba)\n",
    "       # print(pre_label)\n",
    "        return proba#,pre_label\n",
    "    '''\n",
    "    def predict_label(self,proba):\n",
    "        pre_label=np.atgmax(proba,axis=1)  \n",
    "        return pre_label\n",
    "        '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344dc83c",
   "metadata": {},
   "source": [
    "### 2.Choose GaussianNB as the base classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c664dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_GB=AdaClassifier(base_estimator= GaussianNB(),n_estimators=10, learning_rate=0.1, algorithm= 'SAMME.R', random_state=None)\n",
    "#(probability=True),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b31912c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.AdaClassifier at 0x172e9e4d760>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eab5764a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.AdaClassifier at 0x172e9e4d760>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_GB.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4df89750",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-a5d77228151c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel_GB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_samme_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#model,1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model_GB._samme_proba(model,10,X_train)#model,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57442f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pre=model_GB.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e660e231",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pre#output probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5924ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_label=np.argmax(y_pre,axis=1) #Take the category with the highest probability of prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5071df0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90fe746",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19e74b9",
   "metadata": {},
   "source": [
    "### 3.Choose SVC as the base classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1798dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_SVC=AdaClassifier(base_estimator= SVC(probability=True),n_estimators=10, learning_rate=0.1, algorithm= 'SAMME.R', random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8bf534",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_SVC.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606009e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pre=model_SVC.predict_proba(X_test)\n",
    "y_label=np.argmax(y_pre,axis=1)\n",
    "print(classification_report(y_test, y_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228c677f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
